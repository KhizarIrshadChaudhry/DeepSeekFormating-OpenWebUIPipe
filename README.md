# Prepend Think Tag Pipeline

A **2â€‘minute dropâ€‘in** that makes any OpenAIâ€‘compatible backend (vLLM, Ollama, LMÂ Studioâ€¯â€¦) work with OpenWebUIâ€™s **â€œThinkingâ€¦ â³â€** animation.

---

## ğŸ“‚Â Install

```bash
mkdir -p ~/.local/share/open-webui/plugins/functions/
cp -r prepend_think_pipeline ~/.local/share/open-webui/plugins/functions/
# (the folder must contain __init__.py with the Pipe class you dropped earlier)
```

Restart OpenWebUI (or click **Settings â–¸ Plugins â–¸ Reload**).

---

## âš™ï¸Â Configure once

1. **Settings â–¸ Models â–¸ â• Add model** â†’ *OpenAIâ€‘Compatible*
2. **Base URL** â†’ `http://YOUR-VLLM-HOST:8000/v1`
3. **Model name** â†’ whatever you passed to `vllm serve` (e.g. `deepseek-r1-7b-chat`)
4. **Functions & Filters** â†’ tick **Prepend Think Tag Pipeline**
5. *(Optional)* paste an APIÂ key if your server enforces one.

Save â†’ Done.

---

## ğŸš€Â Use

Pick the model in chat and send a prompt. If the model forgets to start with `<think>`, the pipe inserts it automatically, so OpenWebUI shows the **Thinkingâ€¦** placeholder and collapsible chainâ€‘ofâ€‘thought.

Thatâ€™s all - good luck.


